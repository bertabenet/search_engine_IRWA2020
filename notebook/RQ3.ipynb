{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../search-engine')\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from igraph import *\n",
    "import implicit\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import ndcg_score\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB3 - Link analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data \n",
    "Read tweets from data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_tweets(None, None, mode = \"read\", data_directory = '../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph will be created using `igraph` library. The graph will contain only users that have retweeted tweets from other users that also appear in the data collection. Main characteristics are: \n",
    "- Direted graph: A -> B means user A retweeted a tweet from user B. \n",
    "- Nodes: Users\n",
    "- Edges: Retweets. Not weighted graph: meaning that, if user A retweeted three times user B, the number of retweets will not be represented in the graph, only the connection between users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: `igraph` graphs do not contain labels for nodes, therefore, there needs to be a mapping between users' id and the id of the node in the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between user id and node id in the graph.\n",
    "ids_mapping = {}\n",
    "counter = 0\n",
    "for tweet in data:\n",
    "    if \"retweeted_status\" in tweet.keys():\n",
    "        if tweet[\"user\"][\"id\"] not in ids_mapping.keys():\n",
    "            ids_mapping[tweet[\"user\"][\"id\"]] = counter            \n",
    "            counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a dictionary containing as key the node id of the users that retweeted at least a tweet and as value, the node id of the users that wrote the original tweets.\n",
    "\n",
    "An additional dictionary with the hashtags of each tweet (having as key the node id of the users) that will be needed later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a dictionaries\n",
    "retweets = {}\n",
    "hashtags = {} \n",
    "\n",
    "for i, tweet in enumerate(data): \n",
    "    try: \n",
    "        # Store relations between users that have retweeted at least one tweet\n",
    "        id2 = ids_mapping[tweet[\"retweeted_status\"][\"user\"][\"id\"]]        \n",
    "        id1 = ids_mapping[tweet[\"user\"][\"id\"]]\n",
    "        try: retweets[id1].append(id2)\n",
    "        except: retweets[id1] = [id2]\n",
    "        \n",
    "        if id1 not in hashtags.keys(): hashtags[id1] = []\n",
    "        if id2 not in hashtags.keys(): hashtags[id2] = []\n",
    "        \n",
    "        # Store hashtags list for each user for personalized score\n",
    "        for hashtag in tweet[\"entities\"][\"hashtags\"]:\n",
    "            hashtags[id1].append(hashtag[\"text\"])\n",
    "            hashtags[id2].append(hashtag[\"text\"])\n",
    "    except: \n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform hashtags dictionary into a list for the creation of the graph\n",
    "edgelist = []\n",
    "for node in retweets:\n",
    "    for i in retweets[node]:\n",
    "        edgelist.append((node, i))\n",
    "        \n",
    "edgelist = list(set(edgelist)) # Remove duplicated links (since the graph is not weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph(n = len(ids_mapping))\n",
    "g.add_edges(edgelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Networks based predictions\n",
    "Prepare the dataset: split graph into train and test, 80-20% respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2       # fraction of edges to select as test-set\n",
    "N = len(g.es) # graphsize\n",
    "\n",
    "all_idxs = range(N) # idxs of all the edges\n",
    "\n",
    "# sample randomly test nodes \n",
    "np.random.seed(0)\n",
    "test_idxs = np.random.choice(a=all_idxs, size=int(p*N),replace=False)\n",
    "\n",
    "# Create the train-set and ground truth for the test set \n",
    "ground_truth = set() # links between nodes of the test set\n",
    "trainset = set()     # links between nodes of the train set\n",
    "\n",
    "for idx, one_edge in enumerate(g.es):    \n",
    "    n1 = one_edge.source\n",
    "    n2 = one_edge.target\n",
    "\n",
    "    if idx in test_idxs:\n",
    "        ground_truth.add((n1, n2))\n",
    "    else:\n",
    "        trainset.add((n1, n2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance 2 \n",
    "The goal of this part of the project is to predict the probability of having a node for those users at distance 2. \n",
    "\n",
    "Therefore, we need to consider those nodes that are at distance 2, to predict if there's a node between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select nodes at distance 2 (they can also may be at distance 1)\n",
    "def find_nodes_at_distance_2(graph):\n",
    "    all_potential_recommendations = set()\n",
    "    \n",
    "    for n1 in graph.vs:\n",
    "        # all the nodes at distance 1 and distance 2\n",
    "        nodes_at_most_distant_2 = set(graph.neighborhood(n1, order = 2))\n",
    "            \n",
    "        if len(nodes_at_most_distant_2) > 0:\n",
    "            for n2 in nodes_at_most_distant_2: \n",
    "                n1_index = n1.index\n",
    "                if n2 != n1_index: \n",
    "                    a = min(n2, n1_index)\n",
    "                    b = max(n2, n1_index)\n",
    "                    all_potential_recommendations.add((a,b))\n",
    "    return list(set(all_potential_recommendations))\n",
    "\n",
    "# Get nodes at distance 2 in test set\n",
    "dist_2 = find_nodes_at_distance_2(g)\n",
    "test_nodes = [(u,v) for u, v in dist_2 if (u in test_idxs) and (v in test_idxs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe containing the ground truth for the test set. 0 means they have no direct link, while 1 means they are connected. \n",
    "\n",
    "NOTE: there is a path of length at most 2 between the pairs of nodes we are considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5951</td>\n",
       "      <td>13583</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3908</td>\n",
       "      <td>10085</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325</td>\n",
       "      <td>8039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1156</td>\n",
       "      <td>6255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14735</td>\n",
       "      <td>14835</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       u      v  ground_truth\n",
       "0   5951  13583             0\n",
       "1   3908  10085             0\n",
       "2    325   8039             0\n",
       "3   1156   6255             0\n",
       "4  14735  14835             0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe with ground truth\n",
    "test_list = []\n",
    "for u, v in test_nodes: \n",
    "    if (u, v) in ground_truth: test_list.append((u,v,1))\n",
    "    else: test_list.append((u,v,0))\n",
    "        \n",
    "test_df = pd.DataFrame(test_list, columns = [\"u\", \"v\", \"ground_truth\"])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Adamic-Adar (ADA)\n",
    "In this section we will use Adamic-Adar to compute the probability of having a direct link between all nodes that are at most at distance 2. \n",
    "This algorithm does not require a training set, it can be directly applied to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ADA(u,v, graph):\n",
    "    # set of neighbors of u\n",
    "    outlinks_from_u = set(graph.neighbors(u))\n",
    "    # set of neighbors of v\n",
    "    inlinks_to_v = set(graph.neighbors(v))\n",
    "    \n",
    "    # set Z of neighbors of both\n",
    "    bridges = outlinks_from_u.intersection(inlinks_to_v)\n",
    "    \n",
    "    # degree of nodes in set Z\n",
    "    deg_ = [graph.degree(node) for node in bridges]\n",
    "    \n",
    "    # computing the reciprocal in log-scale\n",
    "    out = [1./np.log2(dd+1) for dd in deg_]\n",
    "\n",
    "    return sum(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>ADA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5951</td>\n",
       "      <td>13583</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3908</td>\n",
       "      <td>10085</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>325</td>\n",
       "      <td>8039</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1156</td>\n",
       "      <td>6255</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14735</td>\n",
       "      <td>14835</td>\n",
       "      <td>0</td>\n",
       "      <td>0.127667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       u      v  ground_truth       ADA\n",
       "0   5951  13583             0  0.095358\n",
       "1   3908  10085             0  0.095358\n",
       "2    325   8039             0  0.113583\n",
       "3   1156   6255             0  0.172195\n",
       "4  14735  14835             0  0.127667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the probability\n",
    "pred_ADA = []\n",
    "# Predictions\n",
    "for i, row in test_df.iterrows():\n",
    "    pred = compute_ADA(int(row[\"u\"]), int(row[\"v\"]), g)\n",
    "    pred_ADA.append(pred)\n",
    "    \n",
    "test_df[\"ADA\"] = pred_ADA # Append probabilities to the dataframe\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Alternating Least Squares (ALS)\n",
    "This algorithm has been tested using `implicit` libary. It requires an adjacency matrix (a sparse one). And the approximated predictions are obtained from the dot product of the low-dimensionality vectors obtained from the matrix factorization performed by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the adjacency matrix data\n",
    "M = g.get_adjacency().data\n",
    "M = csr_matrix(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model ALS\n",
    "model = implicit.als.AlternatingLeastSquares(factors=10, calculate_training_loss=True, iterations=5)\n",
    "\n",
    "# Train the model on a sparse matrix\n",
    "model.fit(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link prediction\n",
    "def predict_ALS(testset, model):\n",
    "    all_predictions = []\n",
    "    for n1, n2, w in testset:\n",
    "        # take low-dimensional vectors returned by the matrix factorization\n",
    "        array_n1 = model.user_factors[n1, :]\n",
    "        array_n2 = model.item_factors[n2, :]\n",
    "\n",
    "        # approximation for the edge score\n",
    "        one_p = np.dot(array_n1, array_n2)\n",
    "        all_predictions.append(one_p)\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "all_predictions = predict_ALS(test_df[[\"u\", \"v\", \"ground_truth\"]].values, model)\n",
    "\n",
    "test_df[\"ALS\"] = all_predictions # Append predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Personalized PageRank (PPageRank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library `igraph` has implemented a personalized page rank computation. In personalized page rank, the starting node needs to be specified.\n",
    "\n",
    "\n",
    "For each pair of nodes (A, B) in the graph, the probability of having a link between them is computed by computing the PPageRank starting from A and getting the probability for B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since PPageRank is performed given a graph, all edges from outside the testset should\n",
    "# be dropeed. \n",
    "new_graph = deepcopy(g)\n",
    "new_graph.delete_vertices([i for i in range(len(ids_mapping)) if i not in test_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppage_rank(graph, test_df):\n",
    "    probab = []\n",
    "    # For each pair of nodes, compute their probability.\n",
    "    for u, v in test_df[[\"u\", \"v\"]].values: \n",
    "        probab.append(graph.personalized_pagerank(reset_vertices=u)[v])\n",
    "    return probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"PPageRank\"] = ppage_rank(g, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Personalized score\n",
    "**Main idea**\n",
    "\n",
    "Improve ADA algorithm by adding a partial score to the one returned by ADA algorithm. \n",
    "The partial score considers information from the texts: the hashtags that each user has used. By doing so, we can relate the users having into account their common interests. \n",
    "\n",
    "The dictionary contianing the hashtags used by each user has been created at the beginning of section 2. \n",
    "\n",
    "\n",
    "**Personalized score**\n",
    "\n",
    "\\begin{equation*} \\mathbf{diversity\\_score = ADA + \\lambda * partial\\_score} \\end{equation*}\n",
    "Where: \n",
    "\n",
    "- $\\lambda = 1/3 * max(ADA)$ regulating the impact on the score \n",
    "\n",
    "- $ partial\\_score = Jaccard(hashtags\\_userA, hashtags\\_userB)$\n",
    "\n",
    "By considering the Jaccard similarity between the sets of hashtags used by both users, we are selecting the keywords that the users themselves have selected to summarize the content they are posting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard similarity\n",
    "def compute_Jaccard(u,v):\n",
    "    if len(set(u).union(set(v))) == 0:\n",
    "        return 0.0\n",
    "    return len(set(u).intersection(set(v)))/len(set(u).union(set(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalized_score = []\n",
    "lamb = 1/3\n",
    "max_ADA = test_df[\"ADA\"].max()\n",
    "\n",
    "# Compute personalized score\n",
    "for i, row in test_df.iterrows():\n",
    "    pred = compute_Jaccard(hashtags[int(row[\"u\"])], hashtags[int(row[\"v\"])])\n",
    "    personalized_score.append(row[\"ADA\"] + lamb * max_ADA * pred)\n",
    "    \n",
    "# Add it to the test dataframe    \n",
    "test_df[\"Personalized\"] = personalized_score \n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictions\n",
    "Top-10 predictions for each algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk (scores, column, topk = 10):\n",
    "    scores = scores.sort_values(by=column, ascending = False)\n",
    "    return scores[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"ADA\", \"ALS\", \"PPageRank\", \"Personalized\"]\n",
    "for algorithm in algorithms:\n",
    "    print(\"\\n\\nTop-10 recommendations from {} algoritm\".format(algorithm))\n",
    "    print(get_topk(test_df, algorithm)[[\"u\", \"v\", algorithm, \"ground_truth\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scores comparison - nDCG and precision@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nDCG**\n",
    "\n",
    "\n",
    "nDCG is used to asses the performance of ranking algorithms for categorical scores. It assumes that the added value of an element decreases as it is placed in lower positions of the ranking. \n",
    "\n",
    "To compute nDCG, we used a scoring from 0 to 3, being 3 the best score. \n",
    "For the ground trugh, all 1 (indicating a link) have been replaced with the top score 3, while 0s (indication no direct link) have been left as the worst score 0. \n",
    "\n",
    "For the output of the algorithms, they have been mapped into the scale from 0 to 3 taking into account their minimum and maximum values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform continuous scores to categorical ones (from 0 to 3) being 3 the best score. \n",
    "\n",
    "def compute_intervals(df, column, n):\n",
    "    intervals = []\n",
    "    minimum = df[column].min()\n",
    "    maximum = df[column].max()\n",
    "    interval_length = (maximum - minimum) / (n + 1)\n",
    "    \n",
    "    for i in range(n + 1):\n",
    "        intervals.append((minimum + interval_length * i, minimum + interval_length * (i+1)))\n",
    "    return intervals\n",
    "\n",
    "def substitution (x, intervals):\n",
    "    for i, pos in enumerate(intervals): \n",
    "        if x >= pos[0] and x <= pos[1]:\n",
    "            return i\n",
    "        \n",
    "ranked_scores = test_df.copy()\n",
    "\n",
    "ranked_scores[\"ground_truth\"] = ranked_scores[\"ground_truth\"].apply(lambda x: 3 if x == 1 else 0)\n",
    "ranked_scores[\"ADA\"] = ranked_scores[\"ADA\"].apply(lambda x: substitution(x, compute_intervals(ranked_scores,\"ADA\",3)))\n",
    "ranked_scores[\"ALS\"] = ranked_scores[\"ALS\"].apply(lambda x: substitution(x, compute_intervals(ranked_scores,\"ALS\",3)))\n",
    "ranked_scores[\"PPageRank\"] = ranked_scores[\"PPageRank\"].apply(lambda x: substitution(x, compute_intervals(ranked_scores,\"PPageRank\",3)))\n",
    "ranked_scores[\"Personalized\"] = ranked_scores[\"Personalized\"].apply(lambda x: substitution(x, compute_intervals(ranked_scores,\"Personalized\",3)))\n",
    "\n",
    "ranked_scores.sort_values(by=\"ground_truth\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nDCG scores computation for each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(y_true, y_score,  k=10):\n",
    "    order = np.argsort(y_score)[::-1] # get the list of indexes of the predicted score sorted in descending order.\n",
    "    y_true = np.take(list(y_true), order[:k]) # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2**(y_true) - 1 # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2) # Compute denominator\n",
    "    return np.sum(gain / discounts) #return dcg@k\n",
    "\n",
    "\n",
    "def ndcg_at_k(y_true, y_score, k=10):    \n",
    "    dcg_max = dcg_at_k(y_true, y_true, k) # Ideal dcg\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(dcg_at_k(y_true, y_score, k) / dcg_max,4)  # return ndcg@k\n",
    "\n",
    "def compute_ndcg(df, column, topk = 20): \n",
    "    sorted_ = df.sort_values(by = column, ascending = False)\n",
    "    return ndcg_at_k(sorted_[\"ground_truth\"], sorted_[column], topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for algorithm in algorithms:\n",
    "    print(\"nDCG score ranking with {} algorithm: {}\".format(algorithm, compute_ndcg(ranked_scores, algorithm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision@k**\n",
    "\n",
    "p@20 score has also been used to compare the performance of the algorithms, by checking whether the top-20 link connections recommended by each algorithm are in the actual ground truth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k (scores, column, topk = 20):\n",
    "    scores = get_topk(scores, column, topk)\n",
    "    return (scores[\"ground_truth\"].sum())/(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "for algorithm in algorithms:\n",
    "    print(\"Precision@{} of {} algorithm: {}\".format(k, algorithm, precision_at_k(test_df, algorithm, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
